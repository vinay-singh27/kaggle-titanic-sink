{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(style='ggplot')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "table.dataframe td, table.dataframe th {\n",
       "    border: 1px  black solid !important;\n",
       "  color: black !important;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style type=\"text/css\">\n",
    "table.dataframe td, table.dataframe th {\n",
    "    border: 1px  black solid !important;\n",
    "  color: black !important;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading training data\n",
    "df_train = pd.read_csv(r'C:\\Users\\Vinay\\Documents\\Kaggle\\Titanic_Sink\\train.csv')\n",
    "\n",
    "#Reading test data\n",
    "df_test = pd.read_csv(r'C:\\Users\\Vinay\\Documents\\Kaggle\\Titanic_Sink\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining train and test values\n",
    "data = pd.concat((df_train, df_test)).reset_index(drop=True)\n",
    "x_Survived = df_train[\"Survived\"]\n",
    "data.drop([\"Survived\"], axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('PassengerId', axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass                                               Name     Sex   Age  \\\n",
       "0       3                            Braund, Mr. Owen Harris    male  22.0   \n",
       "1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n",
       "2       3                             Heikkinen, Miss. Laina  female  26.0   \n",
       "3       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0   \n",
       "4       3                           Allen, Mr. William Henry    male  35.0   \n",
       "\n",
       "   SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1      1      0          PC 17599  71.2833   C85        C  \n",
       "2      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      1      0            113803  53.1000  C123        S  \n",
       "4      0      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work in Progress for Fmailty Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              Braund, Mr. Owen Harris\n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...\n",
       "2                               Heikkinen, Miss. Laina\n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)\n",
       "4                             Allen, Mr. William Henry\n",
       "5                                     Moran, Mr. James\n",
       "6                              McCarthy, Mr. Timothy J\n",
       "7                       Palsson, Master. Gosta Leonard\n",
       "8    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\n",
       "9                  Nasser, Mrs. Nicholas (Adele Achem)\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Name'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Family'] = df_train['Name'].apply(lambda x : x.split(',')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Andersson    9\n",
       "Sage         7\n",
       "Panula       6\n",
       "Johnson      6\n",
       "Goodwin      6\n",
       "            ..\n",
       "Reynaldo     1\n",
       "Behr         1\n",
       "Pinsky       1\n",
       "Aks          1\n",
       "Turkula      1\n",
       "Name: Family, Length: 667, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Family'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Family_data = df_train.groupby('Family').sum()['Survived']/df_train.groupby('Family').count()['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Family_data = pd.DataFrame(Family_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Family_data.reset_index(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Family_data.rename(columns= {'Survived' : 'Family_Sur_rate'}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Family</th>\n",
       "      <th>Family_Sur_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbing</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abbott</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abelson</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adahl</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adams</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Family  Family_Sur_rate\n",
       "0   Abbing              0.0\n",
       "1   Abbott              0.5\n",
       "2  Abelson              0.5\n",
       "3    Adahl              0.0\n",
       "4    Adams              0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Family_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Braund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>Cumings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Heikkinen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>Futrelle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Allen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass                                               Name     Sex   Age  \\\n",
       "0       3                            Braund, Mr. Owen Harris    male  22.0   \n",
       "1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n",
       "2       3                             Heikkinen, Miss. Laina  female  26.0   \n",
       "3       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0   \n",
       "4       3                           Allen, Mr. William Henry    male  35.0   \n",
       "\n",
       "   SibSp  Parch            Ticket     Fare Cabin Embarked     Family  \n",
       "0      1      0         A/5 21171   7.2500   NaN        S     Braund  \n",
       "1      1      0          PC 17599  71.2833   C85        C    Cumings  \n",
       "2      0      0  STON/O2. 3101282   7.9250   NaN        S  Heikkinen  \n",
       "3      1      0            113803  53.1000  C123        S   Futrelle  \n",
       "4      0      0            373450   8.0500   NaN        S      Allen  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, Family_data, on='Family')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Family</th>\n",
       "      <th>Family_Sur_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Braund</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Lewis Richard</td>\n",
       "      <td>male</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3460</td>\n",
       "      <td>7.0458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Braund</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>Cumings</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mr. John Bradley</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>Cumings</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Heikkinen</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass                                               Name     Sex   Age  \\\n",
       "0       3                            Braund, Mr. Owen Harris    male  22.0   \n",
       "1       3                          Braund, Mr. Lewis Richard    male  29.0   \n",
       "2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n",
       "3       1                          Cumings, Mr. John Bradley    male  39.0   \n",
       "4       3                             Heikkinen, Miss. Laina  female  26.0   \n",
       "\n",
       "   SibSp  Parch            Ticket     Fare Cabin Embarked     Family  \\\n",
       "0      1      0         A/5 21171   7.2500   NaN        S     Braund   \n",
       "1      1      0              3460   7.0458   NaN        S     Braund   \n",
       "2      1      0          PC 17599  71.2833   C85        C    Cumings   \n",
       "3      1      0          PC 17599  71.2833   C85        C    Cumings   \n",
       "4      0      0  STON/O2. 3101282   7.9250   NaN        S  Heikkinen   \n",
       "\n",
       "   Family_Sur_rate  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              1.0  \n",
       "3              1.0  \n",
       "4              1.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1079 entries, 0 to 1078\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Pclass           1079 non-null   int64  \n",
      " 1   Name             1079 non-null   object \n",
      " 2   Sex              1079 non-null   object \n",
      " 3   Age              873 non-null    float64\n",
      " 4   SibSp            1079 non-null   int64  \n",
      " 5   Parch            1079 non-null   int64  \n",
      " 6   Ticket           1079 non-null   object \n",
      " 7   Fare             1079 non-null   float64\n",
      " 8   Cabin            249 non-null    object \n",
      " 9   Embarked         1077 non-null   object \n",
      " 10  Family           1079 non-null   object \n",
      " 11  Family_Sur_rate  1079 non-null   float64\n",
      "dtypes: float64(3), int64(3), object(6)\n",
      "memory usage: 109.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract title\n",
    "def extract_title(name) :\n",
    "    string = name.split(',')[1].split(' ')[1]\n",
    "    if '.' in string :\n",
    "        return string[:-1]\n",
    "    else :\n",
    "        return string\n",
    "    \n",
    "#function to map other titles to standard title\n",
    "def get_title(title, sex) :\n",
    "    \n",
    "    if title not in ['Mr', 'Mrs', 'Master', 'Miss'] :\n",
    "        \n",
    "        if title == 'the' :\n",
    "            #The Countess implies we can use Mrs can be used\n",
    "            return 'Mrs'\n",
    "        elif title == 'Jonkheer':\n",
    "            #The Jonkheer implies we can use Mr can be used\n",
    "            return 'Mr'\n",
    "        elif title == 'Mlle':\n",
    "            return 'Miss'\n",
    "        elif title == 'Ms':\n",
    "            return 'Miss'\n",
    "        elif title == 'Lady':\n",
    "            return 'Mrs'\n",
    "        else :\n",
    "            if sex == 'male' :\n",
    "                return 'Mr'\n",
    "            else :\n",
    "                return 'Mrs'\n",
    "    else:\n",
    "        return title\n",
    "        \n",
    "#impute age from title\n",
    "def get_age(age, title) :\n",
    "    \n",
    "    if age != age :\n",
    "        if title == 'Master' :\n",
    "            return 5.00\n",
    "        elif title == 'Miss'  :\n",
    "            return 22.00\n",
    "        elif title == 'Mr' :\n",
    "            return 33.00\n",
    "        else:\n",
    "            return 36.00\n",
    "    else :\n",
    "        return age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Title'] = data['Name'].apply(extract_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Title'] = data.apply(lambda x : get_title(x.Title, x.Sex), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age'] = data.apply(lambda x : get_age(x.Age, x.Title), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Family</th>\n",
       "      <th>Family_Sur_rate</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked, Family, Family_Sur_rate, Title]\n",
       "Index: []"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['Fare'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Family</th>\n",
       "      <th>Family_Sur_rate</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "      <td>Icard, Miss. Amelie</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113572</td>\n",
       "      <td>80.0</td>\n",
       "      <td>B28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Icard</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>1</td>\n",
       "      <td>Stone, Mrs. George Nelson (Martha Evelyn)</td>\n",
       "      <td>female</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113572</td>\n",
       "      <td>80.0</td>\n",
       "      <td>B28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stone</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Mrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Pclass                                       Name     Sex   Age  SibSp  \\\n",
       "148        1                        Icard, Miss. Amelie  female  38.0      0   \n",
       "1034       1  Stone, Mrs. George Nelson (Martha Evelyn)  female  62.0      0   \n",
       "\n",
       "      Parch  Ticket  Fare Cabin Embarked Family  Family_Sur_rate Title  \n",
       "148       0  113572  80.0   B28      NaN  Icard              1.0  Miss  \n",
       "1034      0  113572  80.0   B28      NaN  Stone              1.0   Mrs  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['Embarked'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass  Embarked\n",
       "1       C           115.816893\n",
       "        Q            90.000000\n",
       "        S            73.495327\n",
       "2       C            26.250200\n",
       "        Q            12.350000\n",
       "        S            21.406784\n",
       "3       C            11.662354\n",
       "        Q            11.070466\n",
       "        S            15.244716\n",
       "Name: Fare, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['Pclass', 'Embarked']).mean()['Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[1043, 'Fare'] = 14.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[61, 'Embarked'] = 'S'\n",
    "data.loc[829, 'Embarked'] = 'S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Name_Length'] = data['Name'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Name', 'Ticket'], axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1079 entries, 0 to 1078\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Pclass           1079 non-null   int64  \n",
      " 1   Sex              1079 non-null   object \n",
      " 2   Age              1079 non-null   float64\n",
      " 3   SibSp            1079 non-null   int64  \n",
      " 4   Parch            1079 non-null   int64  \n",
      " 5   Fare             1079 non-null   float64\n",
      " 6   Cabin            249 non-null    object \n",
      " 7   Embarked         1077 non-null   object \n",
      " 8   Family           1079 non-null   object \n",
      " 9   Family_Sur_rate  1079 non-null   float64\n",
      " 10  Title            1079 non-null   object \n",
      " 11  Name_Length      1079 non-null   int64  \n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 149.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['Sex', 'Title']\n",
    "\n",
    "features = pd.get_dummies(data[categorical_features], drop_first= True)\n",
    "\n",
    "data = pd.concat([data.drop(categorical_features, axis=1), features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Cabin', axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Family</th>\n",
       "      <th>Family_Sur_rate</th>\n",
       "      <th>Name_Length</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Braund</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0458</td>\n",
       "      <td>S</td>\n",
       "      <td>Braund</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>Cumings</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass   Age  SibSp  Parch     Fare Embarked   Family  Family_Sur_rate  \\\n",
       "0       3  22.0      1      0   7.2500        S   Braund              0.0   \n",
       "1       3  29.0      1      0   7.0458        S   Braund              0.0   \n",
       "2       1  38.0      1      0  71.2833        C  Cumings              1.0   \n",
       "\n",
       "   Name_Length  Sex_male  Title_Miss  Title_Mr  Title_Mrs  \n",
       "0           23         1           0         1          0  \n",
       "1           25         1           0         1          0  \n",
       "2           51         0           0         0          1  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Family'] = data['SibSp'] + data['Parch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Alone'] = data['Family'].apply(lambda x: 1 if x == 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Embarked'] = data['Embarked'].map({'C': 1, 'Q': 2, 'S': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Family</th>\n",
       "      <th>Family_Sur_rate</th>\n",
       "      <th>Name_Length</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0458</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass   Age  SibSp  Parch     Fare  Embarked  Family  Family_Sur_rate  \\\n",
       "0       3  22.0      1      0   7.2500       3.0       2              0.0   \n",
       "1       3  29.0      1      0   7.0458       3.0       2              0.0   \n",
       "2       1  38.0      1      0  71.2833       1.0       2              1.0   \n",
       "\n",
       "   Name_Length  Sex_male  Title_Miss  Title_Mr  Title_Mrs  Alone  \n",
       "0           23         1           0         1          0      0  \n",
       "1           25         1           0         1          0      0  \n",
       "2           51         0           0         0          1      0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['SibSp', 'Parch', 'Family'], axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Family_Sur_rate</th>\n",
       "      <th>Name_Length</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.0458</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass   Age     Fare  Embarked  Family_Sur_rate  Name_Length  Sex_male  \\\n",
       "0       3  22.0   7.2500       3.0              0.0           23         1   \n",
       "1       3  29.0   7.0458       3.0              0.0           25         1   \n",
       "2       1  38.0  71.2833       1.0              1.0           51         0   \n",
       "\n",
       "   Title_Miss  Title_Mr  Title_Mrs  Alone  \n",
       "0           0         1          0      0  \n",
       "1           0         1          0      0  \n",
       "2           0         0          1      0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1079 entries, 0 to 1078\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Pclass           1079 non-null   int64  \n",
      " 1   Age              1079 non-null   float64\n",
      " 2   Fare             1079 non-null   float64\n",
      " 3   Embarked         1077 non-null   float64\n",
      " 4   Family_Sur_rate  1079 non-null   float64\n",
      " 5   Name_Length      1079 non-null   int64  \n",
      " 6   Sex_male         1079 non-null   uint8  \n",
      " 7   Title_Miss       1079 non-null   uint8  \n",
      " 8   Title_Mr         1079 non-null   uint8  \n",
      " 9   Title_Mrs        1079 non-null   uint8  \n",
      " 10  Alone            1079 non-null   int64  \n",
      "dtypes: float64(4), int64(3), uint8(4)\n",
      "memory usage: 111.7 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinay\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_df = data.iloc[:891,:]  \n",
    "train_df['Survived'] = x_Survived\n",
    "test_df = data.iloc[891 :,:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Train & Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('Survived', axis=1)\n",
    "y_train = train_df['Survived']\n",
    "X_test = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = min_max_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = min_max_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, f1_score, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vs_validation_error(model) :\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    predict_train = model.predict(X_train)\n",
    "    print(\"Training Data\")\n",
    "    print(confusion_matrix(y_train, predict_train))\n",
    "    print(f'Precision Score: {precision_score(y_train, predict_train)}')\n",
    "    print(f'Recall Score: {recall_score(y_train, predict_train)}')\n",
    "    print(f'F1 Score: {f1_score(y_train, predict_train)}')\n",
    "    \n",
    "    print('-'*50)\n",
    "    \n",
    "    print(\"Validation Data\")\n",
    "    predict_validation = cross_val_predict(model, X_train, y_train, cv = 10)\n",
    "    y_score = cross_val_predict(model, X_train, y_train, cv = 5, method= \"predict_proba\")\n",
    "    y_proba_score = y_score[:,1]\n",
    "    print(\"Validation Confusion Matrix\")\n",
    "    print(confusion_matrix(y_train, predict_validation))\n",
    "    print(f'Precision Score: {precision_score(y_train, predict_validation)}')\n",
    "    print(f'Recall Score: {recall_score(y_train, predict_validation)}')\n",
    "    print(f'F1 Score: {f1_score(y_train, predict_validation)}')\n",
    "    \n",
    "    print('-'*50)\n",
    "    print(f'AUC: {roc_auc_score(y_train, y_proba_score)}')\n",
    "    \n",
    "    fpr_model, tpr_model, threshold = roc_curve(y_train, y_proba_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "[[548   1]\n",
      " [  3 339]]\n",
      "Precision Score: 0.9970588235294118\n",
      "Recall Score: 0.9912280701754386\n",
      "F1 Score: 0.9941348973607038\n",
      "--------------------------------------------------\n",
      "Validation Data\n",
      "Validation Confusion Matrix\n",
      "[[484  65]\n",
      " [102 240]]\n",
      "Precision Score: 0.7868852459016393\n",
      "Recall Score: 0.7017543859649122\n",
      "F1 Score: 0.7418856259659969\n",
      "--------------------------------------------------\n",
      "AUC: 0.8518252218280981\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "train_vs_validation_error(random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-60ea5a6b7a1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrandom_forest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_vs_validation_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_forest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-75-011d8cd1ddd7>\u001b[0m in \u001b[0;36mtrain_vs_validation_error\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_vs_validation_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mpredict_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \"\"\"\n\u001b[0;32m    294\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 578\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                     (type_err,\n\u001b[1;32m---> 60\u001b[1;33m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[0;32m     61\u001b[0m             )\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "train_vs_validation_error(random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "[[548   1]\n",
      " [  2 340]]\n",
      "Precision Score: 0.9970674486803519\n",
      "Recall Score: 0.9941520467836257\n",
      "F1 Score: 0.9956076134699854\n",
      "--------------------------------------------------\n",
      "Validation Data\n",
      "Validation Confusion Matrix\n",
      "[[481  68]\n",
      " [ 93 249]]\n",
      "Precision Score: 0.7854889589905363\n",
      "Recall Score: 0.7280701754385965\n",
      "F1 Score: 0.755690440060698\n",
      "--------------------------------------------------\n",
      "AUC: 0.8615691475196796\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "train_vs_validation_error(random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "[[548   1]\n",
      " [  2 340]]\n",
      "Precision Score: 0.9970674486803519\n",
      "Recall Score: 0.9941520467836257\n",
      "F1 Score: 0.9956076134699854\n",
      "--------------------------------------------------\n",
      "Validation Data\n",
      "Validation Confusion Matrix\n",
      "[[487  62]\n",
      " [ 95 247]]\n",
      "Precision Score: 0.7993527508090615\n",
      "Recall Score: 0.7222222222222222\n",
      "F1 Score: 0.7588325652841783\n",
      "--------------------------------------------------\n",
      "AUC: 0.8596757528307716\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "train_vs_validation_error(random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators' : [100, 300, 500], 'max_depth' : [7, 10, 15], 'max_features' : [0.5, 'sqrt', 'log2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(random_forest, param_grid= param_grid, cv= 5, verbose= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[CV] max_depth=7, max_features=0.5, n_estimators=100 .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=7, max_features=0.5, n_estimators=100, score=0.827, total=   0.2s\n",
      "[CV] max_depth=7, max_features=0.5, n_estimators=100 .................\n",
      "[CV]  max_depth=7, max_features=0.5, n_estimators=100, score=0.820, total=   0.2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] max_depth=7, max_features=0.5, n_estimators=100 .................\n",
      "[CV]  max_depth=7, max_features=0.5, n_estimators=100, score=0.831, total=   0.2s\n",
      "[CV] max_depth=7, max_features=0.5, n_estimators=100 .................\n",
      "[CV]  max_depth=7, max_features=0.5, n_estimators=100, score=0.815, total=   0.2s\n",
      "[CV] max_depth=7, max_features=0.5, n_estimators=100 .................\n",
      "[CV]  max_depth=7, max_features=0.5, n_estimators=100, score=0.854, total=   0.2s\n",
      "[CV] max_depth=7, max_features=0.5, n_estimators=300 .................\n",
      "[CV]  max_depth=7, max_features=0.5, n_estimators=300, score=0.827, total=   0.6s\n",
      "[CV] max_depth=7, max_features=0.5, n_estimators=300 .................\n",
      "[CV]  max_depth=7, max_features=0.5, n_estimators=300, score=0.820, total=   0.5s\n",
      "[CV] max_depth=7, max_features=0.5, n_estimators=300 .................\n",
      "[CV]  max_depth=7, max_features=0.5, n_estimators=300, score=0.837, total=   0.6s\n",
      "[CV] max_depth=7, max_features=0.5, n_estimators=300 .................\n",
      "[CV]  max_depth=7, max_features=0.5, n_estimators=300, score=0.809, total=   0.7s\n",
      "[CV] max_depth=7, max_features=0.5, n_estimators=300 .................\n",
      "[CV]  max_depth=7, max_features=0.5, n_estimators=300, score=0.854, total=   0.6s\n",
      "[CV] max_depth=7, max_features=0.5, n_estimators=500 .................\n",
      "[CV]  max_depth=7, max_features=0.5, n_estimators=500, score=0.832, total=   1.1s\n",
      "[CV] max_depth=7, max_features=0.5, n_estimators=500 .................\n",
      "[CV]  max_depth=7, max_features=0.5, n_estimators=500, score=0.815, total=   0.9s\n",
      "[CV] max_depth=7, max_features=0.5, n_estimators=500 .................\n",
      "[CV]  max_depth=7, max_features=0.5, n_estimators=500, score=0.837, total=   0.9s\n",
      "[CV] max_depth=7, max_features=0.5, n_estimators=500 .................\n",
      "[CV]  max_depth=7, max_features=0.5, n_estimators=500, score=0.809, total=   0.9s\n",
      "[CV] max_depth=7, max_features=0.5, n_estimators=500 .................\n",
      "[CV]  max_depth=7, max_features=0.5, n_estimators=500, score=0.854, total=   0.9s\n",
      "[CV] max_depth=7, max_features=sqrt, n_estimators=100 ................\n",
      "[CV]  max_depth=7, max_features=sqrt, n_estimators=100, score=0.827, total=   0.2s\n",
      "[CV] max_depth=7, max_features=sqrt, n_estimators=100 ................\n",
      "[CV]  max_depth=7, max_features=sqrt, n_estimators=100, score=0.809, total=   0.2s\n",
      "[CV] max_depth=7, max_features=sqrt, n_estimators=100 ................\n",
      "[CV]  max_depth=7, max_features=sqrt, n_estimators=100, score=0.831, total=   0.2s\n",
      "[CV] max_depth=7, max_features=sqrt, n_estimators=100 ................\n",
      "[CV]  max_depth=7, max_features=sqrt, n_estimators=100, score=0.792, total=   0.2s\n",
      "[CV] max_depth=7, max_features=sqrt, n_estimators=100 ................\n",
      "[CV]  max_depth=7, max_features=sqrt, n_estimators=100, score=0.860, total=   0.3s\n",
      "[CV] max_depth=7, max_features=sqrt, n_estimators=300 ................\n",
      "[CV]  max_depth=7, max_features=sqrt, n_estimators=300, score=0.827, total=   0.6s\n",
      "[CV] max_depth=7, max_features=sqrt, n_estimators=300 ................\n",
      "[CV]  max_depth=7, max_features=sqrt, n_estimators=300, score=0.831, total=   0.5s\n",
      "[CV] max_depth=7, max_features=sqrt, n_estimators=300 ................\n",
      "[CV]  max_depth=7, max_features=sqrt, n_estimators=300, score=0.831, total=   0.5s\n",
      "[CV] max_depth=7, max_features=sqrt, n_estimators=300 ................\n",
      "[CV]  max_depth=7, max_features=sqrt, n_estimators=300, score=0.798, total=   0.5s\n",
      "[CV] max_depth=7, max_features=sqrt, n_estimators=300 ................\n",
      "[CV]  max_depth=7, max_features=sqrt, n_estimators=300, score=0.871, total=   0.5s\n",
      "[CV] max_depth=7, max_features=sqrt, n_estimators=500 ................\n",
      "[CV]  max_depth=7, max_features=sqrt, n_estimators=500, score=0.821, total=   0.9s\n",
      "[CV] max_depth=7, max_features=sqrt, n_estimators=500 ................\n",
      "[CV]  max_depth=7, max_features=sqrt, n_estimators=500, score=0.820, total=   0.9s\n",
      "[CV] max_depth=7, max_features=sqrt, n_estimators=500 ................\n",
      "[CV]  max_depth=7, max_features=sqrt, n_estimators=500, score=0.837, total=   1.2s\n",
      "[CV] max_depth=7, max_features=sqrt, n_estimators=500 ................\n",
      "[CV]  max_depth=7, max_features=sqrt, n_estimators=500, score=0.798, total=   0.9s\n",
      "[CV] max_depth=7, max_features=sqrt, n_estimators=500 ................\n",
      "[CV]  max_depth=7, max_features=sqrt, n_estimators=500, score=0.860, total=   1.1s\n",
      "[CV] max_depth=7, max_features=log2, n_estimators=100 ................\n",
      "[CV]  max_depth=7, max_features=log2, n_estimators=100, score=0.821, total=   0.2s\n",
      "[CV] max_depth=7, max_features=log2, n_estimators=100 ................\n",
      "[CV]  max_depth=7, max_features=log2, n_estimators=100, score=0.820, total=   0.2s\n",
      "[CV] max_depth=7, max_features=log2, n_estimators=100 ................\n",
      "[CV]  max_depth=7, max_features=log2, n_estimators=100, score=0.826, total=   0.2s\n",
      "[CV] max_depth=7, max_features=log2, n_estimators=100 ................\n",
      "[CV]  max_depth=7, max_features=log2, n_estimators=100, score=0.792, total=   0.2s\n",
      "[CV] max_depth=7, max_features=log2, n_estimators=100 ................\n",
      "[CV]  max_depth=7, max_features=log2, n_estimators=100, score=0.854, total=   0.2s\n",
      "[CV] max_depth=7, max_features=log2, n_estimators=300 ................\n",
      "[CV]  max_depth=7, max_features=log2, n_estimators=300, score=0.810, total=   0.5s\n",
      "[CV] max_depth=7, max_features=log2, n_estimators=300 ................\n",
      "[CV]  max_depth=7, max_features=log2, n_estimators=300, score=0.826, total=   0.5s\n",
      "[CV] max_depth=7, max_features=log2, n_estimators=300 ................\n",
      "[CV]  max_depth=7, max_features=log2, n_estimators=300, score=0.837, total=   0.5s\n",
      "[CV] max_depth=7, max_features=log2, n_estimators=300 ................\n",
      "[CV]  max_depth=7, max_features=log2, n_estimators=300, score=0.798, total=   0.7s\n",
      "[CV] max_depth=7, max_features=log2, n_estimators=300 ................\n",
      "[CV]  max_depth=7, max_features=log2, n_estimators=300, score=0.865, total=   0.5s\n",
      "[CV] max_depth=7, max_features=log2, n_estimators=500 ................\n",
      "[CV]  max_depth=7, max_features=log2, n_estimators=500, score=0.844, total=   0.9s\n",
      "[CV] max_depth=7, max_features=log2, n_estimators=500 ................\n",
      "[CV]  max_depth=7, max_features=log2, n_estimators=500, score=0.826, total=   0.9s\n",
      "[CV] max_depth=7, max_features=log2, n_estimators=500 ................\n",
      "[CV]  max_depth=7, max_features=log2, n_estimators=500, score=0.831, total=   0.9s\n",
      "[CV] max_depth=7, max_features=log2, n_estimators=500 ................\n",
      "[CV]  max_depth=7, max_features=log2, n_estimators=500, score=0.803, total=   0.9s\n",
      "[CV] max_depth=7, max_features=log2, n_estimators=500 ................\n",
      "[CV]  max_depth=7, max_features=log2, n_estimators=500, score=0.860, total=   0.9s\n",
      "[CV] max_depth=10, max_features=0.5, n_estimators=100 ................\n",
      "[CV]  max_depth=10, max_features=0.5, n_estimators=100, score=0.844, total=   0.3s\n",
      "[CV] max_depth=10, max_features=0.5, n_estimators=100 ................\n",
      "[CV]  max_depth=10, max_features=0.5, n_estimators=100, score=0.831, total=   0.2s\n",
      "[CV] max_depth=10, max_features=0.5, n_estimators=100 ................\n",
      "[CV]  max_depth=10, max_features=0.5, n_estimators=100, score=0.854, total=   0.2s\n",
      "[CV] max_depth=10, max_features=0.5, n_estimators=100 ................\n",
      "[CV]  max_depth=10, max_features=0.5, n_estimators=100, score=0.803, total=   0.2s\n",
      "[CV] max_depth=10, max_features=0.5, n_estimators=100 ................\n",
      "[CV]  max_depth=10, max_features=0.5, n_estimators=100, score=0.854, total=   0.2s\n",
      "[CV] max_depth=10, max_features=0.5, n_estimators=300 ................\n",
      "[CV]  max_depth=10, max_features=0.5, n_estimators=300, score=0.838, total=   0.6s\n",
      "[CV] max_depth=10, max_features=0.5, n_estimators=300 ................\n",
      "[CV]  max_depth=10, max_features=0.5, n_estimators=300, score=0.820, total=   0.6s\n",
      "[CV] max_depth=10, max_features=0.5, n_estimators=300 ................\n",
      "[CV]  max_depth=10, max_features=0.5, n_estimators=300, score=0.843, total=   0.6s\n",
      "[CV] max_depth=10, max_features=0.5, n_estimators=300 ................\n",
      "[CV]  max_depth=10, max_features=0.5, n_estimators=300, score=0.798, total=   0.6s\n",
      "[CV] max_depth=10, max_features=0.5, n_estimators=300 ................\n",
      "[CV]  max_depth=10, max_features=0.5, n_estimators=300, score=0.860, total=   0.6s\n",
      "[CV] max_depth=10, max_features=0.5, n_estimators=500 ................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=10, max_features=0.5, n_estimators=500, score=0.838, total=   1.2s\n",
      "[CV] max_depth=10, max_features=0.5, n_estimators=500 ................\n",
      "[CV]  max_depth=10, max_features=0.5, n_estimators=500, score=0.826, total=   1.2s\n",
      "[CV] max_depth=10, max_features=0.5, n_estimators=500 ................\n",
      "[CV]  max_depth=10, max_features=0.5, n_estimators=500, score=0.843, total=   1.0s\n",
      "[CV] max_depth=10, max_features=0.5, n_estimators=500 ................\n",
      "[CV]  max_depth=10, max_features=0.5, n_estimators=500, score=0.809, total=   1.0s\n",
      "[CV] max_depth=10, max_features=0.5, n_estimators=500 ................\n",
      "[CV]  max_depth=10, max_features=0.5, n_estimators=500, score=0.848, total=   1.0s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=100, score=0.827, total=   0.2s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=100, score=0.820, total=   0.2s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=100, score=0.854, total=   0.2s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=100, score=0.809, total=   0.2s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=100, score=0.865, total=   0.2s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=300 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=300, score=0.832, total=   0.7s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=300 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=300, score=0.837, total=   0.6s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=300 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=300, score=0.854, total=   0.5s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=300 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=300, score=0.809, total=   0.6s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=300 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=300, score=0.865, total=   0.6s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=500, score=0.821, total=   0.9s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=500, score=0.837, total=   0.9s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=500, score=0.843, total=   0.9s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=500, score=0.809, total=   1.1s\n",
      "[CV] max_depth=10, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=sqrt, n_estimators=500, score=0.865, total=   0.9s\n",
      "[CV] max_depth=10, max_features=log2, n_estimators=100 ...............\n",
      "[CV]  max_depth=10, max_features=log2, n_estimators=100, score=0.816, total=   0.2s\n",
      "[CV] max_depth=10, max_features=log2, n_estimators=100 ...............\n",
      "[CV]  max_depth=10, max_features=log2, n_estimators=100, score=0.837, total=   0.2s\n",
      "[CV] max_depth=10, max_features=log2, n_estimators=100 ...............\n",
      "[CV]  max_depth=10, max_features=log2, n_estimators=100, score=0.848, total=   0.2s\n",
      "[CV] max_depth=10, max_features=log2, n_estimators=100 ...............\n",
      "[CV]  max_depth=10, max_features=log2, n_estimators=100, score=0.803, total=   0.2s\n",
      "[CV] max_depth=10, max_features=log2, n_estimators=100 ...............\n",
      "[CV]  max_depth=10, max_features=log2, n_estimators=100, score=0.860, total=   0.2s\n",
      "[CV] max_depth=10, max_features=log2, n_estimators=300 ...............\n",
      "[CV]  max_depth=10, max_features=log2, n_estimators=300, score=0.838, total=   0.6s\n",
      "[CV] max_depth=10, max_features=log2, n_estimators=300 ...............\n",
      "[CV]  max_depth=10, max_features=log2, n_estimators=300, score=0.831, total=   0.5s\n",
      "[CV] max_depth=10, max_features=log2, n_estimators=300 ...............\n",
      "[CV]  max_depth=10, max_features=log2, n_estimators=300, score=0.848, total=   0.6s\n",
      "[CV] max_depth=10, max_features=log2, n_estimators=300 ...............\n",
      "[CV]  max_depth=10, max_features=log2, n_estimators=300, score=0.820, total=   0.6s\n",
      "[CV] max_depth=10, max_features=log2, n_estimators=300 ...............\n",
      "[CV]  max_depth=10, max_features=log2, n_estimators=300, score=0.871, total=   0.8s\n",
      "[CV] max_depth=10, max_features=log2, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=log2, n_estimators=500, score=0.844, total=   0.9s\n",
      "[CV] max_depth=10, max_features=log2, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=log2, n_estimators=500, score=0.826, total=   0.9s\n",
      "[CV] max_depth=10, max_features=log2, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=log2, n_estimators=500, score=0.848, total=   0.9s\n",
      "[CV] max_depth=10, max_features=log2, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=log2, n_estimators=500, score=0.803, total=   0.9s\n",
      "[CV] max_depth=10, max_features=log2, n_estimators=500 ...............\n",
      "[CV]  max_depth=10, max_features=log2, n_estimators=500, score=0.865, total=   0.9s\n",
      "[CV] max_depth=15, max_features=0.5, n_estimators=100 ................\n",
      "[CV]  max_depth=15, max_features=0.5, n_estimators=100, score=0.832, total=   0.3s\n",
      "[CV] max_depth=15, max_features=0.5, n_estimators=100 ................\n",
      "[CV]  max_depth=15, max_features=0.5, n_estimators=100, score=0.809, total=   0.3s\n",
      "[CV] max_depth=15, max_features=0.5, n_estimators=100 ................\n",
      "[CV]  max_depth=15, max_features=0.5, n_estimators=100, score=0.848, total=   0.3s\n",
      "[CV] max_depth=15, max_features=0.5, n_estimators=100 ................\n",
      "[CV]  max_depth=15, max_features=0.5, n_estimators=100, score=0.803, total=   0.3s\n",
      "[CV] max_depth=15, max_features=0.5, n_estimators=100 ................\n",
      "[CV]  max_depth=15, max_features=0.5, n_estimators=100, score=0.831, total=   0.3s\n",
      "[CV] max_depth=15, max_features=0.5, n_estimators=300 ................\n",
      "[CV]  max_depth=15, max_features=0.5, n_estimators=300, score=0.838, total=   0.6s\n",
      "[CV] max_depth=15, max_features=0.5, n_estimators=300 ................\n",
      "[CV]  max_depth=15, max_features=0.5, n_estimators=300, score=0.826, total=   0.6s\n",
      "[CV] max_depth=15, max_features=0.5, n_estimators=300 ................\n",
      "[CV]  max_depth=15, max_features=0.5, n_estimators=300, score=0.843, total=   0.6s\n",
      "[CV] max_depth=15, max_features=0.5, n_estimators=300 ................\n",
      "[CV]  max_depth=15, max_features=0.5, n_estimators=300, score=0.798, total=   0.6s\n",
      "[CV] max_depth=15, max_features=0.5, n_estimators=300 ................\n",
      "[CV]  max_depth=15, max_features=0.5, n_estimators=300, score=0.837, total=   0.6s\n",
      "[CV] max_depth=15, max_features=0.5, n_estimators=500 ................\n",
      "[CV]  max_depth=15, max_features=0.5, n_estimators=500, score=0.827, total=   1.2s\n",
      "[CV] max_depth=15, max_features=0.5, n_estimators=500 ................\n",
      "[CV]  max_depth=15, max_features=0.5, n_estimators=500, score=0.826, total=   1.0s\n",
      "[CV] max_depth=15, max_features=0.5, n_estimators=500 ................\n",
      "[CV]  max_depth=15, max_features=0.5, n_estimators=500, score=0.843, total=   1.0s\n",
      "[CV] max_depth=15, max_features=0.5, n_estimators=500 ................\n",
      "[CV]  max_depth=15, max_features=0.5, n_estimators=500, score=0.798, total=   1.0s\n",
      "[CV] max_depth=15, max_features=0.5, n_estimators=500 ................\n",
      "[CV]  max_depth=15, max_features=0.5, n_estimators=500, score=0.826, total=   1.1s\n",
      "[CV] max_depth=15, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=15, max_features=sqrt, n_estimators=100, score=0.816, total=   0.2s\n",
      "[CV] max_depth=15, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=15, max_features=sqrt, n_estimators=100, score=0.820, total=   0.3s\n",
      "[CV] max_depth=15, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=15, max_features=sqrt, n_estimators=100, score=0.848, total=   0.2s\n",
      "[CV] max_depth=15, max_features=sqrt, n_estimators=100 ...............\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=15, max_features=sqrt, n_estimators=100, score=0.792, total=   0.4s\n",
      "[CV] max_depth=15, max_features=sqrt, n_estimators=100 ...............\n",
      "[CV]  max_depth=15, max_features=sqrt, n_estimators=100, score=0.848, total=   0.3s\n",
      "[CV] max_depth=15, max_features=sqrt, n_estimators=300 ...............\n",
      "[CV]  max_depth=15, max_features=sqrt, n_estimators=300, score=0.821, total=   0.5s\n",
      "[CV] max_depth=15, max_features=sqrt, n_estimators=300 ...............\n",
      "[CV]  max_depth=15, max_features=sqrt, n_estimators=300, score=0.826, total=   0.6s\n",
      "[CV] max_depth=15, max_features=sqrt, n_estimators=300 ...............\n",
      "[CV]  max_depth=15, max_features=sqrt, n_estimators=300, score=0.837, total=   0.6s\n",
      "[CV] max_depth=15, max_features=sqrt, n_estimators=300 ...............\n",
      "[CV]  max_depth=15, max_features=sqrt, n_estimators=300, score=0.792, total=   0.6s\n",
      "[CV] max_depth=15, max_features=sqrt, n_estimators=300 ...............\n",
      "[CV]  max_depth=15, max_features=sqrt, n_estimators=300, score=0.837, total=   0.6s\n",
      "[CV] max_depth=15, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=15, max_features=sqrt, n_estimators=500, score=0.821, total=   0.9s\n",
      "[CV] max_depth=15, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=15, max_features=sqrt, n_estimators=500, score=0.826, total=   0.9s\n",
      "[CV] max_depth=15, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=15, max_features=sqrt, n_estimators=500, score=0.843, total=   1.2s\n",
      "[CV] max_depth=15, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=15, max_features=sqrt, n_estimators=500, score=0.803, total=   0.9s\n",
      "[CV] max_depth=15, max_features=sqrt, n_estimators=500 ...............\n",
      "[CV]  max_depth=15, max_features=sqrt, n_estimators=500, score=0.837, total=   0.9s\n",
      "[CV] max_depth=15, max_features=log2, n_estimators=100 ...............\n",
      "[CV]  max_depth=15, max_features=log2, n_estimators=100, score=0.816, total=   0.2s\n",
      "[CV] max_depth=15, max_features=log2, n_estimators=100 ...............\n",
      "[CV]  max_depth=15, max_features=log2, n_estimators=100, score=0.826, total=   0.2s\n",
      "[CV] max_depth=15, max_features=log2, n_estimators=100 ...............\n",
      "[CV]  max_depth=15, max_features=log2, n_estimators=100, score=0.843, total=   0.2s\n",
      "[CV] max_depth=15, max_features=log2, n_estimators=100 ...............\n",
      "[CV]  max_depth=15, max_features=log2, n_estimators=100, score=0.792, total=   0.2s\n",
      "[CV] max_depth=15, max_features=log2, n_estimators=100 ...............\n",
      "[CV]  max_depth=15, max_features=log2, n_estimators=100, score=0.854, total=   0.2s\n",
      "[CV] max_depth=15, max_features=log2, n_estimators=300 ...............\n",
      "[CV]  max_depth=15, max_features=log2, n_estimators=300, score=0.838, total=   0.6s\n",
      "[CV] max_depth=15, max_features=log2, n_estimators=300 ...............\n",
      "[CV]  max_depth=15, max_features=log2, n_estimators=300, score=0.820, total=   0.6s\n",
      "[CV] max_depth=15, max_features=log2, n_estimators=300 ...............\n",
      "[CV]  max_depth=15, max_features=log2, n_estimators=300, score=0.843, total=   0.8s\n",
      "[CV] max_depth=15, max_features=log2, n_estimators=300 ...............\n",
      "[CV]  max_depth=15, max_features=log2, n_estimators=300, score=0.803, total=   0.6s\n",
      "[CV] max_depth=15, max_features=log2, n_estimators=300 ...............\n",
      "[CV]  max_depth=15, max_features=log2, n_estimators=300, score=0.848, total=   0.6s\n",
      "[CV] max_depth=15, max_features=log2, n_estimators=500 ...............\n",
      "[CV]  max_depth=15, max_features=log2, n_estimators=500, score=0.821, total=   0.9s\n",
      "[CV] max_depth=15, max_features=log2, n_estimators=500 ...............\n",
      "[CV]  max_depth=15, max_features=log2, n_estimators=500, score=0.831, total=   0.9s\n",
      "[CV] max_depth=15, max_features=log2, n_estimators=500 ...............\n",
      "[CV]  max_depth=15, max_features=log2, n_estimators=500, score=0.843, total=   1.0s\n",
      "[CV] max_depth=15, max_features=log2, n_estimators=500 ...............\n",
      "[CV]  max_depth=15, max_features=log2, n_estimators=500, score=0.792, total=   1.3s\n",
      "[CV] max_depth=15, max_features=log2, n_estimators=500 ...............\n",
      "[CV]  max_depth=15, max_features=log2, n_estimators=500, score=0.854, total=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 135 out of 135 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'max_depth': [7, 10, 15],\n",
       "                         'max_features': [0.5, 'sqrt', 'log2'],\n",
       "                         'n_estimators': [100, 300, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=3)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10, 'max_features': 'log2', 'n_estimators': 300}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results = grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8294080723118448 {'max_depth': 7, 'max_features': 0.5, 'n_estimators': 100}\n",
      "0.829408072311845 {'max_depth': 7, 'max_features': 0.5, 'n_estimators': 300}\n",
      "0.8294017952419811 {'max_depth': 7, 'max_features': 0.5, 'n_estimators': 500}\n",
      "0.8237900947837551 {'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.8316552633230808 {'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 300}\n",
      "0.8271671583704727 {'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "0.8226727763480008 {'max_depth': 7, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.8271797125102003 {'max_depth': 7, 'max_features': 'log2', 'n_estimators': 300}\n",
      "0.8327600276191074 {'max_depth': 7, 'max_features': 'log2', 'n_estimators': 500}\n",
      "0.8372544096415794 {'max_depth': 10, 'max_features': 0.5, 'n_estimators': 100}\n",
      "0.8316427091833531 {'max_depth': 10, 'max_features': 0.5, 'n_estimators': 300}\n",
      "0.8327663046889711 {'max_depth': 10, 'max_features': 0.5, 'n_estimators': 500}\n",
      "0.8350260498399347 {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.8395141547925429 {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 300}\n",
      "0.8350323269097985 {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "0.8327914129684263 {'max_depth': 10, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.8417550687339149 {'max_depth': 10, 'max_features': 'log2', 'n_estimators': 300}\n",
      "0.8372544096415794 {'max_depth': 10, 'max_features': 'log2', 'n_estimators': 500}\n",
      "0.8249074132195092 {'max_depth': 15, 'max_features': 0.5, 'n_estimators': 100}\n",
      "0.8282719226664993 {'max_depth': 15, 'max_features': 0.5, 'n_estimators': 300}\n",
      "0.8237900947837549 {'max_depth': 15, 'max_features': 0.5, 'n_estimators': 500}\n",
      "0.8249262444291006 {'max_depth': 15, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.8226727763480008 {'max_depth': 15, 'max_features': 'sqrt', 'n_estimators': 300}\n",
      "0.8260435628648548 {'max_depth': 15, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "0.8260498399347185 {'max_depth': 15, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.8305191136777352 {'max_depth': 15, 'max_features': 'log2', 'n_estimators': 300}\n",
      "0.8282907538760907 {'max_depth': 15, 'max_features': 'log2', 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "for mean_score, param in zip(grid_results['mean_test_score'], grid_results['params']) :\n",
    "    print(mean_score, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "features = pd.DataFrame(data= feature_importance.reshape(1,-1), columns= X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Name_Length</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Family</th>\n",
       "      <th>Alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.09128</td>\n",
       "      <td>0.131807</td>\n",
       "      <td>0.164467</td>\n",
       "      <td>0.027914</td>\n",
       "      <td>0.137288</td>\n",
       "      <td>0.110488</td>\n",
       "      <td>0.035812</td>\n",
       "      <td>0.181698</td>\n",
       "      <td>0.036148</td>\n",
       "      <td>0.072655</td>\n",
       "      <td>0.010443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pclass       Age      Fare  Embarked  Name_Length  Sex_male  Title_Miss  \\\n",
       "0  0.09128  0.131807  0.164467  0.027914     0.137288  0.110488    0.035812   \n",
       "\n",
       "   Title_Mr  Title_Mrs    Family     Alone  \n",
       "0  0.181698   0.036148  0.072655  0.010443  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_random_forest = RandomForestClassifier(max_depth= 10, max_features= 'log2', n_estimators= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "[[545   4]\n",
      " [ 39 303]]\n",
      "Precision Score: 0.9869706840390879\n",
      "Recall Score: 0.8859649122807017\n",
      "F1 Score: 0.9337442218798152\n",
      "--------------------------------------------------\n",
      "Validation Data\n",
      "Validation Confusion Matrix\n",
      "[[498  51]\n",
      " [ 96 246]]\n",
      "Precision Score: 0.8282828282828283\n",
      "Recall Score: 0.7192982456140351\n",
      "F1 Score: 0.7699530516431925\n",
      "--------------------------------------------------\n",
      "AUC: 0.8677047049925969\n"
     ]
    }
   ],
   "source": [
    "train_vs_validation_error(tuned_random_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "[[544   5]\n",
      " [ 12 330]]\n",
      "Precision Score: 0.9850746268656716\n",
      "Recall Score: 0.9649122807017544\n",
      "F1 Score: 0.9748892171344165\n",
      "--------------------------------------------------\n",
      "Validation Data\n",
      "Validation Confusion Matrix\n",
      "[[484  65]\n",
      " [107 235]]\n",
      "Precision Score: 0.7833333333333333\n",
      "Recall Score: 0.6871345029239766\n",
      "F1 Score: 0.7320872274143303\n",
      "--------------------------------------------------\n",
      "AUC: 0.8472608357566656\n"
     ]
    }
   ],
   "source": [
    "bagging = BaggingClassifier()\n",
    "\n",
    "train_vs_validation_error(bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "[[518  31]\n",
      " [ 60 282]]\n",
      "Precision Score: 0.9009584664536742\n",
      "Recall Score: 0.8245614035087719\n",
      "F1 Score: 0.8610687022900764\n",
      "--------------------------------------------------\n",
      "Validation Data\n",
      "Validation Confusion Matrix\n",
      "[[490  59]\n",
      " [103 239]]\n",
      "Precision Score: 0.802013422818792\n",
      "Recall Score: 0.6988304093567251\n",
      "F1 Score: 0.7468750000000001\n",
      "--------------------------------------------------\n",
      "AUC: 0.8672866136196593\n"
     ]
    }
   ],
   "source": [
    "gradient_boosting = GradientBoostingClassifier()\n",
    "\n",
    "train_vs_validation_error(gradient_boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "[[514  35]\n",
      " [ 67 275]]\n",
      "Precision Score: 0.8870967741935484\n",
      "Recall Score: 0.804093567251462\n",
      "F1 Score: 0.843558282208589\n",
      "--------------------------------------------------\n",
      "Validation Data\n",
      "Validation Confusion Matrix\n",
      "[[490  59]\n",
      " [104 238]]\n",
      "Precision Score: 0.8013468013468014\n",
      "Recall Score: 0.695906432748538\n",
      "F1 Score: 0.7449139280125195\n",
      "--------------------------------------------------\n",
      "AUC: 0.8688817520425228\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgboost = XGBClassifier()\n",
    "\n",
    "train_vs_validation_error(xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class XGBClassifier in module xgboost.sklearn:\n",
      "\n",
      "class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      " |  XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      " |  \n",
      " |  Implementation of the scikit-learn API for XGBoost classification.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  max_depth : int\n",
      " |      Maximum tree depth for base learners.\n",
      " |  learning_rate : float\n",
      " |      Boosting learning rate (xgb's \"eta\")\n",
      " |  n_estimators : int\n",
      " |      Number of trees to fit.\n",
      " |  verbosity : int\n",
      " |      The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      " |  silent : boolean\n",
      " |      Whether to print messages while running boosting. Deprecated. Use verbosity instead.\n",
      " |  objective : string or callable\n",
      " |      Specify the learning task and the corresponding learning objective or\n",
      " |      a custom objective function to be used (see note below).\n",
      " |  booster: string\n",
      " |      Specify which booster to use: gbtree, gblinear or dart.\n",
      " |  nthread : int\n",
      " |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      " |  n_jobs : int\n",
      " |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      " |  gamma : float\n",
      " |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      " |  min_child_weight : int\n",
      " |      Minimum sum of instance weight(hessian) needed in a child.\n",
      " |  max_delta_step : int\n",
      " |      Maximum delta step we allow each tree's weight estimation to be.\n",
      " |  subsample : float\n",
      " |      Subsample ratio of the training instance.\n",
      " |  colsample_bytree : float\n",
      " |      Subsample ratio of columns when constructing each tree.\n",
      " |  colsample_bylevel : float\n",
      " |      Subsample ratio of columns for each level.\n",
      " |  colsample_bynode : float\n",
      " |      Subsample ratio of columns for each split.\n",
      " |  reg_alpha : float (xgb's alpha)\n",
      " |      L1 regularization term on weights\n",
      " |  reg_lambda : float (xgb's lambda)\n",
      " |      L2 regularization term on weights\n",
      " |  scale_pos_weight : float\n",
      " |      Balancing of positive and negative weights.\n",
      " |  base_score:\n",
      " |      The initial prediction score of all instances, global bias.\n",
      " |  seed : int\n",
      " |      Random number seed.  (Deprecated, please use random_state)\n",
      " |  random_state : int\n",
      " |      Random number seed.  (replaces seed)\n",
      " |  missing : float, optional\n",
      " |      Value in the data which needs to be present as a missing value. If\n",
      " |      None, defaults to np.nan.\n",
      " |  importance_type: string, default \"gain\"\n",
      " |      The feature importance type for the feature_importances_ property: either \"gain\",\n",
      " |      \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      " |  \\*\\*kwargs : dict, optional\n",
      " |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      " |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      " |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      " |      will result in a TypeError.\n",
      " |  \n",
      " |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      " |  \n",
      " |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      " |          passed via this argument will interact properly with scikit-learn.\n",
      " |  \n",
      " |  Note\n",
      " |  ----\n",
      " |  A custom objective function can be provided for the ``objective``\n",
      " |  parameter. In this case, it should have the signature\n",
      " |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      " |  \n",
      " |  y_true: array_like of shape [n_samples]\n",
      " |      The target values\n",
      " |  y_pred: array_like of shape [n_samples]\n",
      " |      The predicted values\n",
      " |  \n",
      " |  grad: array_like of shape [n_samples]\n",
      " |      The value of the gradient for each sample point.\n",
      " |  hess: array_like of shape [n_samples]\n",
      " |      The value of the second derivative for each sample point\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBClassifier\n",
      " |      XGBModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  evals_result(self)\n",
      " |      Return the evaluation results.\n",
      " |      \n",
      " |      If **eval_set** is passed to the `fit` function, you can call\n",
      " |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      " |      When **eval_metric** is also passed to the `fit` function, the\n",
      " |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      evals_result : dictionary\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      " |      \n",
      " |          clf = xgb.XGBClassifier(**param_dist)\n",
      " |      \n",
      " |          clf.fit(X_train, y_train,\n",
      " |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      " |                  eval_metric='logloss',\n",
      " |                  verbose=True)\n",
      " |      \n",
      " |          evals_result = clf.evals_result()\n",
      " |      \n",
      " |      The variable **evals_result** will contain\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      " |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      " |      Fit gradient boosting classifier\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Feature matrix\n",
      " |      y : array_like\n",
      " |          Labels\n",
      " |      sample_weight : array_like\n",
      " |          Weight for each instance\n",
      " |      eval_set : list, optional\n",
      " |          A list of (X, y) pairs to use as a validation set for\n",
      " |          early-stopping\n",
      " |      sample_weight_eval_set : list, optional\n",
      " |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      " |          instance weights on the i-th validation set.\n",
      " |      eval_metric : str, callable, optional\n",
      " |          If a str, should be a built-in evaluation metric to use. See\n",
      " |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      " |          signature is func(y_predicted, y_true) where y_true will be a\n",
      " |          DMatrix object such that you may need to call the get_label\n",
      " |          method. It must return a str, value pair where the str is a name\n",
      " |          for the evaluation and value is the value of the evaluation\n",
      " |          function. This objective is always minimized.\n",
      " |      early_stopping_rounds : int, optional\n",
      " |          Activates early stopping. Validation error needs to decrease at\n",
      " |          least every <early_stopping_rounds> round(s) to continue training.\n",
      " |          Requires at least one item in evals. If there's more than one,\n",
      " |          will use the last. If early stopping occurs, the model will have\n",
      " |          three additional fields: bst.best_score, bst.best_iteration and\n",
      " |          bst.best_ntree_limit (bst.best_ntree_limit is the ntree_limit parameter\n",
      " |          default value in predict method if not any other value is specified).\n",
      " |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      " |          and/or num_class appears in the parameters)\n",
      " |      verbose : bool\n",
      " |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      " |          metric measured on the validation set to stderr.\n",
      " |      xgb_model : str\n",
      " |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      " |          loaded before training (allows training continuation).\n",
      " |      callbacks : list of callback functions\n",
      " |          List of callback functions that are applied at end of each iteration.\n",
      " |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      " |          Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      " |  \n",
      " |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      " |      Predict with `data`.\n",
      " |      \n",
      " |      .. note:: This function is not thread safe.\n",
      " |      \n",
      " |        For each booster object, predict can only be called from one thread.\n",
      " |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      " |        of model object and then call ``predict()``.\n",
      " |      \n",
      " |      .. note:: Using ``predict()`` with DART booster\n",
      " |      \n",
      " |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      " |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      " |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      " |        a nonzero value, e.g.\n",
      " |      \n",
      " |        .. code-block:: python\n",
      " |      \n",
      " |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : DMatrix\n",
      " |          The dmatrix storing the input.\n",
      " |      output_margin : bool\n",
      " |          Whether to output the raw untransformed margin value.\n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      " |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      " |      validate_features : bool\n",
      " |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      " |          Otherwise, it is assumed that the feature_names are the same.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : numpy array\n",
      " |  \n",
      " |  predict_proba(self, data, ntree_limit=None, validate_features=True)\n",
      " |      Predict the probability of each `data` example being of a given class.\n",
      " |      \n",
      " |      .. note:: This function is not thread safe\n",
      " |      \n",
      " |          For each booster object, predict can only be called from one thread.\n",
      " |          If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      " |          of model object and then call predict\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : DMatrix\n",
      " |          The dmatrix storing the input.\n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      " |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      " |      validate_features : bool\n",
      " |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      " |          Otherwise, it is assumed that the feature_names are the same.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : numpy array\n",
      " |          a numpy array with the probability of each data example being of a given class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from XGBModel:\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  apply(self, X, ntree_limit=0)\n",
      " |      Return the predicted leaf every tree for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      " |          For each datapoint x in X and for each tree, return the index of the\n",
      " |          leaf x ends up in. Leaves are numbered within\n",
      " |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      " |  \n",
      " |  get_booster(self)\n",
      " |      Get the underlying xgboost Booster of this model.\n",
      " |      \n",
      " |      This will raise an exception when fit was not called\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster : a xgboost booster of underlying model\n",
      " |  \n",
      " |  get_num_boosting_rounds(self)\n",
      " |      Gets the number of xgboost boosting rounds.\n",
      " |  \n",
      " |  get_params(self, deep=False)\n",
      " |      Get parameters.\n",
      " |  \n",
      " |  get_xgb_params(self)\n",
      " |      Get xgboost type parameters.\n",
      " |  \n",
      " |  load_model(self, fname)\n",
      " |      Load the model from a file.\n",
      " |      \n",
      " |      The model is loaded from an XGBoost internal binary format which is\n",
      " |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      " |      the Python Booster object (such as feature names) will not be loaded.\n",
      " |      Label encodings (text labels to numeric labels) will be also lost.\n",
      " |      **If you are using only the Python interface, we recommend pickling the\n",
      " |      model object for best results.**\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or a memory buffer\n",
      " |          Input file name or memory buffer(see also save_raw)\n",
      " |  \n",
      " |  save_model(self, fname)\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      The model is saved in an XGBoost internal binary format which is\n",
      " |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      " |      the Python Booster object (such as feature names) will not be loaded.\n",
      " |      Label encodings (text labels to numeric labels) will be also lost.\n",
      " |      **If you are using only the Python interface, we recommend pickling the\n",
      " |      model object for best results.**\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Output file name\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      " |      the full range of xgboost parameters that are not defined as member variables\n",
      " |      in sklearn grid search.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from XGBModel:\n",
      " |  \n",
      " |  coef_\n",
      " |      Coefficients property\n",
      " |      \n",
      " |      .. note:: Coefficients are defined only for linear learners\n",
      " |      \n",
      " |          Coefficients are only defined when the linear model is chosen as base\n",
      " |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      " |          as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Feature importances property\n",
      " |      \n",
      " |      .. note:: Feature importance is defined only for tree boosters\n",
      " |      \n",
      " |          Feature importance is only defined when the decision tree model is chosen as base\n",
      " |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      " |          as linear learners (`booster=gblinear`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array of shape ``[n_features]``\n",
      " |  \n",
      " |  intercept_\n",
      " |      Intercept (bias) property\n",
      " |      \n",
      " |      .. note:: Intercept is defined only for linear learners\n",
      " |      \n",
      " |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      " |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      " |          as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_xg = {'n_estimators' : [300, 500, 1000, 1500],  'learning_rate' : [ 0.02, 0.05,1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_xgboost = GridSearchCV(xgboost, param_grid= param_grid_xg, cv = 5, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] learning_rate=0.02, n_estimators=300 ............................\n",
      "[CV]  learning_rate=0.02, n_estimators=300, score=0.788, total=   0.2s\n",
      "[CV] learning_rate=0.02, n_estimators=300 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.02, n_estimators=300, score=0.820, total=   0.2s\n",
      "[CV] learning_rate=0.02, n_estimators=300 ............................\n",
      "[CV]  learning_rate=0.02, n_estimators=300, score=0.831, total=   0.1s\n",
      "[CV] learning_rate=0.02, n_estimators=300 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.02, n_estimators=300, score=0.798, total=   0.2s\n",
      "[CV] learning_rate=0.02, n_estimators=300 ............................\n",
      "[CV]  learning_rate=0.02, n_estimators=300, score=0.854, total=   0.2s\n",
      "[CV] learning_rate=0.02, n_estimators=500 ............................\n",
      "[CV]  learning_rate=0.02, n_estimators=500, score=0.821, total=   0.2s\n",
      "[CV] learning_rate=0.02, n_estimators=500 ............................\n",
      "[CV]  learning_rate=0.02, n_estimators=500, score=0.826, total=   0.3s\n",
      "[CV] learning_rate=0.02, n_estimators=500 ............................\n",
      "[CV]  learning_rate=0.02, n_estimators=500, score=0.826, total=   0.2s\n",
      "[CV] learning_rate=0.02, n_estimators=500 ............................\n",
      "[CV]  learning_rate=0.02, n_estimators=500, score=0.815, total=   0.2s\n",
      "[CV] learning_rate=0.02, n_estimators=500 ............................\n",
      "[CV]  learning_rate=0.02, n_estimators=500, score=0.848, total=   0.3s\n",
      "[CV] learning_rate=0.02, n_estimators=1000 ...........................\n",
      "[CV]  learning_rate=0.02, n_estimators=1000, score=0.827, total=   0.5s\n",
      "[CV] learning_rate=0.02, n_estimators=1000 ...........................\n",
      "[CV]  learning_rate=0.02, n_estimators=1000, score=0.831, total=   0.5s\n",
      "[CV] learning_rate=0.02, n_estimators=1000 ...........................\n",
      "[CV]  learning_rate=0.02, n_estimators=1000, score=0.848, total=   0.5s\n",
      "[CV] learning_rate=0.02, n_estimators=1000 ...........................\n",
      "[CV]  learning_rate=0.02, n_estimators=1000, score=0.826, total=   0.5s\n",
      "[CV] learning_rate=0.02, n_estimators=1000 ...........................\n",
      "[CV]  learning_rate=0.02, n_estimators=1000, score=0.854, total=   0.9s\n",
      "[CV] learning_rate=0.02, n_estimators=1500 ...........................\n",
      "[CV]  learning_rate=0.02, n_estimators=1500, score=0.832, total=   0.7s\n",
      "[CV] learning_rate=0.02, n_estimators=1500 ...........................\n",
      "[CV]  learning_rate=0.02, n_estimators=1500, score=0.820, total=   0.7s\n",
      "[CV] learning_rate=0.02, n_estimators=1500 ...........................\n",
      "[CV]  learning_rate=0.02, n_estimators=1500, score=0.831, total=   0.7s\n",
      "[CV] learning_rate=0.02, n_estimators=1500 ...........................\n",
      "[CV]  learning_rate=0.02, n_estimators=1500, score=0.837, total=   0.7s\n",
      "[CV] learning_rate=0.02, n_estimators=1500 ...........................\n",
      "[CV]  learning_rate=0.02, n_estimators=1500, score=0.854, total=   0.7s\n",
      "[CV] learning_rate=0.05, n_estimators=300 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=300, score=0.838, total=   0.1s\n",
      "[CV] learning_rate=0.05, n_estimators=300 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=300, score=0.820, total=   0.2s\n",
      "[CV] learning_rate=0.05, n_estimators=300 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=300, score=0.837, total=   0.1s\n",
      "[CV] learning_rate=0.05, n_estimators=300 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=300, score=0.820, total=   0.1s\n",
      "[CV] learning_rate=0.05, n_estimators=300 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=300, score=0.854, total=   0.1s\n",
      "[CV] learning_rate=0.05, n_estimators=500 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=500, score=0.832, total=   0.2s\n",
      "[CV] learning_rate=0.05, n_estimators=500 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=500, score=0.831, total=   0.4s\n",
      "[CV] learning_rate=0.05, n_estimators=500 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=500, score=0.854, total=   0.3s\n",
      "[CV] learning_rate=0.05, n_estimators=500 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=500, score=0.843, total=   0.2s\n",
      "[CV] learning_rate=0.05, n_estimators=500 ............................\n",
      "[CV]  learning_rate=0.05, n_estimators=500, score=0.848, total=   0.2s\n",
      "[CV] learning_rate=0.05, n_estimators=1000 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=1000, score=0.844, total=   0.5s\n",
      "[CV] learning_rate=0.05, n_estimators=1000 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=1000, score=0.831, total=   0.5s\n",
      "[CV] learning_rate=0.05, n_estimators=1000 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=1000, score=0.854, total=   0.4s\n",
      "[CV] learning_rate=0.05, n_estimators=1000 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=1000, score=0.831, total=   0.5s\n",
      "[CV] learning_rate=0.05, n_estimators=1000 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=1000, score=0.837, total=   0.5s\n",
      "[CV] learning_rate=0.05, n_estimators=1500 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=1500, score=0.832, total=   0.7s\n",
      "[CV] learning_rate=0.05, n_estimators=1500 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=1500, score=0.815, total=   0.7s\n",
      "[CV] learning_rate=0.05, n_estimators=1500 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=1500, score=0.843, total=   0.8s\n",
      "[CV] learning_rate=0.05, n_estimators=1500 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=1500, score=0.831, total=   0.8s\n",
      "[CV] learning_rate=0.05, n_estimators=1500 ...........................\n",
      "[CV]  learning_rate=0.05, n_estimators=1500, score=0.820, total=   0.7s\n",
      "[CV] learning_rate=1, n_estimators=300 ...............................\n",
      "[CV] ... learning_rate=1, n_estimators=300, score=0.816, total=   0.1s\n",
      "[CV] learning_rate=1, n_estimators=300 ...............................\n",
      "[CV] ... learning_rate=1, n_estimators=300, score=0.764, total=   0.2s\n",
      "[CV] learning_rate=1, n_estimators=300 ...............................\n",
      "[CV] ... learning_rate=1, n_estimators=300, score=0.826, total=   0.1s\n",
      "[CV] learning_rate=1, n_estimators=300 ...............................\n",
      "[CV] ... learning_rate=1, n_estimators=300, score=0.803, total=   0.1s\n",
      "[CV] learning_rate=1, n_estimators=300 ...............................\n",
      "[CV] ... learning_rate=1, n_estimators=300, score=0.764, total=   0.1s\n",
      "[CV] learning_rate=1, n_estimators=500 ...............................\n",
      "[CV] ... learning_rate=1, n_estimators=500, score=0.821, total=   0.2s\n",
      "[CV] learning_rate=1, n_estimators=500 ...............................\n",
      "[CV] ... learning_rate=1, n_estimators=500, score=0.758, total=   0.2s\n",
      "[CV] learning_rate=1, n_estimators=500 ...............................\n",
      "[CV] ... learning_rate=1, n_estimators=500, score=0.820, total=   0.2s\n",
      "[CV] learning_rate=1, n_estimators=500 ...............................\n",
      "[CV] ... learning_rate=1, n_estimators=500, score=0.809, total=   0.2s\n",
      "[CV] learning_rate=1, n_estimators=500 ...............................\n",
      "[CV] ... learning_rate=1, n_estimators=500, score=0.770, total=   0.2s\n",
      "[CV] learning_rate=1, n_estimators=1000 ..............................\n",
      "[CV] .. learning_rate=1, n_estimators=1000, score=0.827, total=   0.5s\n",
      "[CV] learning_rate=1, n_estimators=1000 ..............................\n",
      "[CV] .. learning_rate=1, n_estimators=1000, score=0.753, total=   0.5s\n",
      "[CV] learning_rate=1, n_estimators=1000 ..............................\n",
      "[CV] .. learning_rate=1, n_estimators=1000, score=0.843, total=   0.4s\n",
      "[CV] learning_rate=1, n_estimators=1000 ..............................\n",
      "[CV] .. learning_rate=1, n_estimators=1000, score=0.809, total=   0.8s\n",
      "[CV] learning_rate=1, n_estimators=1000 ..............................\n",
      "[CV] .. learning_rate=1, n_estimators=1000, score=0.764, total=   0.6s\n",
      "[CV] learning_rate=1, n_estimators=1500 ..............................\n",
      "[CV] .. learning_rate=1, n_estimators=1500, score=0.827, total=   0.8s\n",
      "[CV] learning_rate=1, n_estimators=1500 ..............................\n",
      "[CV] .. learning_rate=1, n_estimators=1500, score=0.764, total=   0.7s\n",
      "[CV] learning_rate=1, n_estimators=1500 ..............................\n",
      "[CV] .. learning_rate=1, n_estimators=1500, score=0.843, total=   0.8s\n",
      "[CV] learning_rate=1, n_estimators=1500 ..............................\n",
      "[CV] .. learning_rate=1, n_estimators=1500, score=0.815, total=   0.7s\n",
      "[CV] learning_rate=1, n_estimators=1500 ..............................\n",
      "[CV] .. learning_rate=1, n_estimators=1500, score=0.775, total=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   24.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=100, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=None, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'learning_rate': [0.02, 0.05, 1],\n",
       "                         'n_estimators': [300, 500, 1000, 1500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=3)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_xgboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.05, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=500, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_xgboost.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_xgboost = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "              learning_rate=0.05, max_delta_step=0, max_depth=3,\n",
    "              min_child_weight=1, missing=None, n_estimators=500, n_jobs=1,\n",
    "              nthread=None, objective='binary:logistic', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "              silent=None, subsample=1, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "[[527  22]\n",
      " [ 47 295]]\n",
      "Precision Score: 0.9305993690851735\n",
      "Recall Score: 0.8625730994152047\n",
      "F1 Score: 0.8952959028831562\n",
      "--------------------------------------------------\n",
      "Validation Data\n",
      "Validation Confusion Matrix\n",
      "[[495  54]\n",
      " [100 242]]\n",
      "Precision Score: 0.8175675675675675\n",
      "Recall Score: 0.7076023391812866\n",
      "F1 Score: 0.7586206896551724\n",
      "--------------------------------------------------\n",
      "AUC: 0.8689137080710276\n"
     ]
    }
   ],
   "source": [
    "train_vs_validation_error(optimized_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.05, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=500, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_xgboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['Pclass', 'Age', 'Fare', 'Embarked', 'Name_Length', 'Sex_male', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Family', 'Alone'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10']\nexpected Name_Length, Title_Mrs, Title_Miss, Pclass, Fare, Alone, Title_Mr, Embarked, Age, Family, Sex_male in input data\ntraining data did not have the following fields: f1, f2, f6, f3, f7, f4, f9, f8, f10, f5, f0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-184-3a8ff7d96d6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimized_xgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, output_margin, ntree_limit, validate_features)\u001b[0m\n\u001b[0;32m    789\u001b[0m                                                  \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m                                                  \u001b[0mntree_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mntree_limit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m                                                  validate_features=validate_features)\n\u001b[0m\u001b[0;32m    792\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[1;31m# If output_margin is active, simply return the scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1284\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m         \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[1;32m-> 1690\u001b[1;33m                                             data.feature_names))\n\u001b[0m\u001b[0;32m   1691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_split_value_histogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: feature_names mismatch: ['Pclass', 'Age', 'Fare', 'Embarked', 'Name_Length', 'Sex_male', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Family', 'Alone'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10']\nexpected Name_Length, Title_Mrs, Title_Miss, Pclass, Fare, Alone, Title_Mr, Embarked, Age, Family, Sex_male in input data\ntraining data did not have the following fields: f1, f2, f6, f3, f7, f4, f9, f8, f10, f5, f0"
     ]
    }
   ],
   "source": [
    "predicts = optimized_xgboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
